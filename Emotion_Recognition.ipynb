{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Emotion_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNo3g0tVabpP"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35gt1D-ya6R6",
        "outputId": "a9e088c7-89a4-444f-b6b6-79e58ed5196a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmYQ80vYa03h"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Emotion Recognition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEEG5BZgabqW",
        "outputId": "a4a56fac-3da2-4f29-aa70-bbf997fb8225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df = pd.read_csv('text_emotion.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>xoshayzers</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>wannamama</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>coolfunky</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>czareaquino</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>xkilljoyx</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     tweet_id  ...                                            content\n",
              "0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n",
              "1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2  1956967696  ...                Funeral ceremony...gloomy friday...\n",
              "3  1956967789  ...               wants to hang out with friends SOON!\n",
              "4  1956968416  ...  @dannycastillo We want to trade with someone w...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVoomDa7abrT",
        "outputId": "c2f63f64-6b7f-4d49-85a1-45c24a34d5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.000000e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.845184e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.188579e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.693956e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.751431e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.855443e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.962781e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.966441e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           tweet_id\n",
              "count  4.000000e+04\n",
              "mean   1.845184e+09\n",
              "std    1.188579e+08\n",
              "min    1.693956e+09\n",
              "25%    1.751431e+09\n",
              "50%    1.855443e+09\n",
              "75%    1.962781e+09\n",
              "max    1.966441e+09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg0XvVrtabrt"
      },
      "source": [
        "### Analyzing different types of emotions present in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQzK8BsLabsF"
      },
      "source": [
        "### Correction of Mispelled words and slangs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "I5Q674jBabsX",
        "outputId": "d279d1c4-8f0e-4800-ddeb-74c913a519d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "correction_df = pd.read_csv(\"Spelling Corrector/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n",
        "correction_df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correction</th>\n",
              "      <th>misspell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>wimp</td>\n",
              "      <td>whimp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>won't</td>\n",
              "      <td>wan't</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>writing</td>\n",
              "      <td>writting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>yield</td>\n",
              "      <td>yeild</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>your</td>\n",
              "      <td>youe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    correction   misspell\n",
              "445       wimp      whimp\n",
              "446      won't      wan't\n",
              "447    writing   writting\n",
              "448      yield      yeild\n",
              "449       your       youe"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNUocMakabtY"
      },
      "source": [
        "correction_df.drop_duplicates(\"misspell\",inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae_9dE6Jabtj",
        "outputId": "e1f1f28d-04b5-43a5-e1ba-932a49ad9231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(correction_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(447, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGotFY15abuf"
      },
      "source": [
        "Mapping the incorrect words to the correct ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7rAVvblabuh",
        "outputId": "0d7644bd-9200-4d76-f35b-23d5906ae29e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "corr_dict = dict(zip(correction_df.misspell, correction_df.correction))\n",
        "#Sample of mapping\n",
        "print(dict(list(corr_dict.items())[0:12]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' nevade': 'Nevada', ' presbyterian': 'Presbyterian', ' rsx': 'RSX', ' Steffen': 'Stephen', ' susan': 'Susan', ' abilitey': 'ability', ' abouy': 'about', ' absorbtion': 'absorption', ' accidently': 'accidentally', ' accomodate acommadate': 'accommodate', ' acord': 'accord', ' aquantance': 'acquaintance'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TojLFlKLabvS",
        "outputId": "f73e3543-2404-4faa-9e6d-44540da6487b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Converting short forms to correct words\n",
        "short = pd.read_csv(\"Spelling Corrector/contractions.csv\")\n",
        "short_dict = dict(zip(short[\"Contraction\"], short[\"Meaning\"]))\n",
        "print(dict(list(short_dict.items())[0:12]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"'aight\": 'alright', \"ain't\": 'is not', \"amn't\": 'am not', \"aren't\": 'are not', \"can't\": 'cannot', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0avd_tKGabvw"
      },
      "source": [
        "#Functions to correct the tweets\n",
        "import re\n",
        "def spell_correction(tweet):\n",
        "    for word in tweet.split():\n",
        "        if word in corr_dict.keys():\n",
        "            tweet = tweet.replace(word,corr_dict[word])\n",
        "    return tweet\n",
        "def full_form(tweet):\n",
        "    for word in tweet.split():\n",
        "        if word in short_dict.keys():\n",
        "            tweet = tweet.replace(word,short_dict[word])\n",
        "    return tweet    \n",
        "def clean_punctuation(tweet):\n",
        "    for word in tweet.lower():\n",
        "        if word in '''()-[]{};:\"\\,<>./@#$%^&_~''':\n",
        "            tweet = tweet.replace(word,\" \")\n",
        "        elif word in \"'\":\n",
        "            tweet = tweet.replace(word,\"\")\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz0-ay8NabwA"
      },
      "source": [
        "!pip install emoji\n",
        "import emoji\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean(df):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        stemmer = PorterStemmer()\n",
        "        df['clean_tweet'] = df['content']\n",
        "        clean_tokens = []\n",
        "        stop_words = stopwords.words('english') \n",
        "        df['clean_tweet'] = df['clean_tweet'].str.replace(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\",\"\")\n",
        "        df['clean_tweet'] = df['clean_tweet'].str.replace(\"@[\\w]*\",\"\")\n",
        "        df['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z0-9' ]\",\" \")\n",
        "        df['clean_tweet'] = df['clean_tweet'].str.replace(\"(^| ).( |$)\",\" \")\n",
        "        df['clean_tweet'] = df['clean_tweet'].str.replace(\"^(RT)\",\"\")\n",
        "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x:spell_correction(x))\n",
        "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x:full_form(x))\n",
        "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x:' '.join(clean_punctuation(emoji.demojize(x)).split()))\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsDmgEs4abwV"
      },
      "source": [
        "df = clean(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE1bAJ90abxO",
        "outputId": "ddc549c9-1258-41ea-be7f-4f0f9552a823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df['clean_tweet'].tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39995                                                     \n",
              "39996                        Happy Mothers Day All my love\n",
              "39997    Happy Mothers Day to all the mommies out there...\n",
              "39998    WASSUP BEAUTIFUL FOLLOW ME PEEP OUT MY NEW HIT...\n",
              "39999    bullet train from tokyo the gf and have been v...\n",
              "Name: clean_tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCFl7ZUrabx8"
      },
      "source": [
        "Drop empty tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ7-8MTRabyB",
        "outputId": "e9aa66c7-0f93-4d1e-ccf6-66435e1d6172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Before : \",df.shape)\n",
        "df = df[df.clean_tweet != \"\"]\n",
        "print(\"After : \",df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before :  (40000, 5)\n",
            "After :  (39902, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJvSGtShabyI",
        "outputId": "f7a47cf7-76f2-435a-f9ac-6816ed79593d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral       8567\n",
              "worry         8454\n",
              "happiness     5208\n",
              "sadness       5162\n",
              "love          3841\n",
              "surprise      2185\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          812\n",
              "enthusiasm     759\n",
              "boredom        179\n",
              "anger          110\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlFBuB8abyY"
      },
      "source": [
        "### Encoding the sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y64nBEJXabyd",
        "outputId": "a936a914-b9be-46e1-edb7-464834a4a960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "sent_to_id  = {\"empty\":0, \"sadness\":1,\"enthusiasm\":2,\"neutral\":3,\"worry\":4,\"surprise\":5,\"love\":6,\"fun\":7,\"hate\":8,\"happiness\":9,\"boredom\":10,\"relief\":11,\"anger\":12}\n",
        "df[\"sentiment_id\"] = df['sentiment'].map(sent_to_id)\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(df['sentiment_id'])\n",
        "labels = labels.reshape(len(labels),1)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [6]\n",
            " [9]\n",
            " [6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeoCvWU7abzt",
        "outputId": "5f88b71b-a6ff-4a72-eb0a-a75db78a026d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "ohe = OneHotEncoder(sparse=False)\n",
        "y = ohe.fit_transform(labels)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4MwUOTabz7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(df['clean_tweet'],y,test_size=0.35,random_state=1,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCGENebEab0L",
        "outputId": "1bf47a4d-aee1-47a3-a91c-1c0ee36a44d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "x_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22022                   got to play Off the Wall first tho\n",
              "12114    are you on about BGT im not watching it tonigh...\n",
              "1316     Waah You were in Aberdeen why do miss all the ...\n",
              "31182           my gut feel says its going to be boy ababa\n",
              "27659                                      Hey Dave whasup\n",
              "Name: clean_tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oRqIZ7Vab0a"
      },
      "source": [
        "### Creating word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvx98llIab0e"
      },
      "source": [
        "#!pip install keras\n",
        "#!pip install tensorflow\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,Embedding,LSTM,Dropout,GRU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hP0OBWxZORk"
      },
      "source": [
        "from keras.layers import Input, Embedding, SpatialDropout1D, LSTM\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import Bidirectional, Conv1D, Dense, concatenate\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMCi6_PBab1N"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INhYhoCqab1R"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "train_sequence = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequence = tokenizer.texts_to_sequences(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODGjB3Y2ab1a",
        "outputId": "49399df3-7334-4dcb-ce18-18b3c8f94f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len = max(len(seq) for seq in train_sequence)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M63byzz4ab2A"
      },
      "source": [
        "train_pad = pad_sequences(train_sequence,maxlen= max_len,padding='pre')\n",
        "test_pad = pad_sequences(test_sequence,maxlen= max_len,padding='pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx1NTi1Lab3K",
        "outputId": "e9f41141-7fa9-4634-c6e1-3568d8da7bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "train_pad[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    4, 2136],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xNRdlvpab3l"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGNHso70ab4U"
      },
      "source": [
        "#### 1. Using Glove Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12rSrFX8ab4W"
      },
      "source": [
        "import numpy as np\n",
        "word2vec = {}\n",
        "vocab = set()\n",
        "with open('glove.6B.200d.txt',\"r\",encoding=\"utf8\") as file:\n",
        "    for line in file:\n",
        "        word = line.strip().split()\n",
        "        vocab.add(word[0])\n",
        "        word2vec[word[0]] =  np.array(word[1:],dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkp562ebab4c",
        "outputId": "26c4c98d-7c20-42b8-e397-c20a92a9f315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Words in Vocabulary : \",len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in Vocabulary :  400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfgyzLXaab4m"
      },
      "source": [
        "embed_matrix = np.zeros((vocab_size,250))\n",
        "for word,index in tokenizer.word_index.items():\n",
        "    embed_vec = tokenizer.word_index.get(word)\n",
        "    if embed_vec is not None:\n",
        "        embed_matrix[index] = embed_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj7RgUc0ab4s",
        "outputId": "e5800c85-0771-4798-a92b-8f52255f527a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(embed_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3-G_o28ab6J",
        "outputId": "c3d7459a-7f40-49c6-e348-c1f9ca92aaf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_pad.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTWP6BHfab6f"
      },
      "source": [
        "#### LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6gYKKDGab6h",
        "outputId": "ee7e87c8-cf81-4950-945e-5ad024ff393b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "model = Sequential()\n",
        "embed = Embedding(vocab_size,250,input_length = train_pad.shape[1],weights=[embed_matrix],trainable=False)\n",
        "model.add(embed)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(250,dropout=0.2,recurrent_dropout=0.2))\n",
        "# model.add(LSTM(250,dropout=0.2,recurrent_dropout=0.2))\n",
        "model.add(Dense(13,activation='softmax'))\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 33, 250)           5892750   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 33, 250)           0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 250)               376500    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 13)                3263      \n",
            "=================================================================\n",
            "Total params: 6,272,513\n",
            "Trainable params: 379,763\n",
            "Non-trainable params: 5,892,750\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvSsTzewab6u",
        "outputId": "710f4952-f301-4ae6-8e18-a7cb8b1b24be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "model.fit(train_pad,y_train,epochs=15,batch_size=128,validation_data=(test_pad,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "203/203 [==============================] - 23s 115ms/step - loss: 2.2132 - accuracy: 0.2175 - val_loss: 2.1751 - val_accuracy: 0.2129\n",
            "Epoch 2/15\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.1558 - accuracy: 0.2244 - val_loss: 2.1648 - val_accuracy: 0.2180\n",
            "Epoch 3/15\n",
            "203/203 [==============================] - 22s 108ms/step - loss: 2.1513 - accuracy: 0.2269 - val_loss: 2.1760 - val_accuracy: 0.2119\n",
            "Epoch 4/15\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.1526 - accuracy: 0.2274 - val_loss: 2.1554 - val_accuracy: 0.2296\n",
            "Epoch 5/15\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.1477 - accuracy: 0.2291 - val_loss: 2.1622 - val_accuracy: 0.2250\n",
            "Epoch 6/15\n",
            "203/203 [==============================] - 23s 111ms/step - loss: 2.1508 - accuracy: 0.2252 - val_loss: 2.1527 - val_accuracy: 0.2268\n",
            "Epoch 7/15\n",
            "203/203 [==============================] - 23s 112ms/step - loss: 2.1472 - accuracy: 0.2297 - val_loss: 2.1580 - val_accuracy: 0.2262\n",
            "Epoch 8/15\n",
            "203/203 [==============================] - 22s 108ms/step - loss: 2.1475 - accuracy: 0.2271 - val_loss: 2.1591 - val_accuracy: 0.2354\n",
            "Epoch 9/15\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.1486 - accuracy: 0.2254 - val_loss: 2.1580 - val_accuracy: 0.2190\n",
            "Epoch 10/15\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.1475 - accuracy: 0.2262 - val_loss: 2.1564 - val_accuracy: 0.2144\n",
            "Epoch 11/15\n",
            "203/203 [==============================] - 22s 108ms/step - loss: 2.1481 - accuracy: 0.2252 - val_loss: 2.1582 - val_accuracy: 0.2384\n",
            "Epoch 12/15\n",
            "203/203 [==============================] - 22s 109ms/step - loss: 2.1481 - accuracy: 0.2282 - val_loss: 2.1529 - val_accuracy: 0.2401\n",
            "Epoch 13/15\n",
            "203/203 [==============================] - 22s 108ms/step - loss: 2.1459 - accuracy: 0.2287 - val_loss: 2.1521 - val_accuracy: 0.2346\n",
            "Epoch 14/15\n",
            "203/203 [==============================] - 22s 108ms/step - loss: 2.1478 - accuracy: 0.2281 - val_loss: 2.1565 - val_accuracy: 0.2255\n",
            "Epoch 15/15\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.1444 - accuracy: 0.2325 - val_loss: 2.1644 - val_accuracy: 0.2126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f60f4337860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScyaqqTMab6z"
      },
      "source": [
        "def get_sentiment(model,text):\n",
        "    text = spell_correction(text)\n",
        "    text = full_form(text)\n",
        "    twt = tokenizer.texts_to_sequences([text])\n",
        "    twt = pad_sequences(twt, maxlen=max_len, dtype='int32')\n",
        "    sentiment = model.predict(twt,batch_size=1,verbose = 2)\n",
        "    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n",
        "    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n",
        "    result.columns = [\"sentiment\",\"percentage\"]\n",
        "    result=result[result.percentage !=0]\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dkVUiKsab67",
        "outputId": "f8ec7186-5171-4299-acc2-7efcaa4925f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "result =get_sentiment(model,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I canâ€™t hold myself back. I really miss you\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 - 0s\n",
            "     sentiment percentage\n",
            "0        empty          2\n",
            "1      sadness         10\n",
            "2   enthusiasm          2\n",
            "3      neutral         21\n",
            "4        worry         21\n",
            "5     surprise          6\n",
            "6         love         14\n",
            "7          fun          2\n",
            "8         hate          3\n",
            "9    happiness         17\n",
            "11      relief          2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLvadtfDaHV5"
      },
      "source": [
        "input_length = train_pad.shape[1]\n",
        "input_dim = vocab_size\n",
        "num_classes = 13\n",
        "embedding_dim = 250\n",
        "lstm_units = 128\n",
        "lstm_dropout = 0.1\n",
        "recurrent_dropout = 0.1\n",
        "spatial_dropout=0.2\n",
        "filters=64\n",
        "kernel_size=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXNZzHwRbSn7",
        "outputId": "04524656-63be-4d3a-e7c9-f264fcdaaa4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "input_layer = Input(shape=(input_length,))\n",
        "output_layer = Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_shape=(input_length,))(input_layer)\n",
        "output_layer = SpatialDropout1D(spatial_dropout)(output_layer)\n",
        "output_layer = Bidirectional(LSTM(units=lstm_units,return_sequences=True,dropout=lstm_dropout,recurrent_dropout=recurrent_dropout))(output_layer)\n",
        "output_layer = Conv1D(filters,kernel_size=kernel_size,kernel_initializer='glorot_uniform')(output_layer)\n",
        "avgPool = GlobalAveragePooling1D()(output_layer)\n",
        "maxPool = GlobalMaxPooling1D()(output_layer)\n",
        "output_layer = concatenate([avgPool,maxPool])\n",
        "output_layer = Dense(num_classes,activation='softmax')(output_layer)\n",
        "model = Model(input_layer,output_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF5-CsapeX0O"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWyi5ELCe2wo"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsax6Rqme67Q",
        "outputId": "9dfaed4a-8fd3-4913-844b-d1d4764d8f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(train_pad,y_train,batch_size=batch_size,epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "406/406 [==============================] - 118s 291ms/step - loss: 2.0131 - accuracy: 0.3049\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 119s 292ms/step - loss: 1.7430 - accuracy: 0.4118\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 118s 290ms/step - loss: 1.4284 - accuracy: 0.5257\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 118s 291ms/step - loss: 1.0602 - accuracy: 0.6578\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 118s 291ms/step - loss: 0.7898 - accuracy: 0.7455\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 117s 289ms/step - loss: 0.6109 - accuracy: 0.7980\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 118s 290ms/step - loss: 0.5013 - accuracy: 0.8347\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 118s 290ms/step - loss: 0.4337 - accuracy: 0.8570\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 119s 292ms/step - loss: 0.3793 - accuracy: 0.8729\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 118s 291ms/step - loss: 0.3406 - accuracy: 0.8866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f60c0ea55c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1tnIzidhvrN",
        "outputId": "9cb9865d-f705-439f-f8d9-72ed540097c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result = get_sentiment(model,\"I was very very upset\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 - 0s\n",
            "    sentiment percentage\n",
            "1     sadness         22\n",
            "3     neutral          4\n",
            "4       worry         48\n",
            "5    surprise          1\n",
            "6        love         16\n",
            "8        hate          1\n",
            "9   happiness          4\n",
            "11     relief          2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}